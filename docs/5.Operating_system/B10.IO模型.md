---

id : B10.IO模型
title : B10.IO模型
typora-root-url : ../
---

## epoll高效原理

`epoll`使用`RB-Tree`红黑树去监听并维护所有文件描述符，`RB-Tree`的根节点

调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个**红黑树**用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件.

当**epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效**。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已.

那么，这个准备就绪list链表是怎么维护的呢？

当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。

epoll相比于select并不是在所有情况下都要高效，例如在如果有少于1024个文件描述符监听，且大多数socket都是出于活跃繁忙的状态，这种情况下，select要比epoll更为高效，因为epoll会有更多次的系统调用，用户态和内核态会有更加频繁的切换。

epoll高效的本质在于：

- 减少了用户态和内核态的文件句柄拷贝
- 减少了对可读可写文件句柄的遍历
- mmap 加速了内核与用户空间的信息传递，epoll是通过内核与用户mmap同一块内存，避免了无谓的内存拷贝
- IO性能不会随着监听的文件描述的数量增长而下降
- 使用红黑树存储fd，以及对应的回调函数，其插入，查找，删除的性能不错，相比于hash，不必预先分配很多的空间



## 多线程与多进程

1. 多线程比多进程，内存相互共享，通信简单
2. 线程中单独模块化去写，通过传入回调函数
3. 线程调度比进程调度小很多

线程安全：竞争的发生，多个线程访问统一资源，需要互斥访问



## fork-io-worker多进程

![image-20210319080347559](/Image/B10.IO模型-photo/image-20210319080347559.png)

多进程服务器,每一次client与acceptor(服务端监听套接字)连接,若判定为有效连接,则fork开一个新进程

```cpp
void worker(int fd, struct User *user) {
    while (1) {
        char buff[512] = {0};
        if ((recv(fd, buff, sizeof(buff), 0)) <= 0) {
            break;
        }
        printf(GREEN"<%s>"NONE" : %s!\n", user->name, buff);
    }
    close(fd);
    DBG(GREEN"<%s>"NONE" : logout!\n", user->name);
    return ;
}

int main(int argc, char **argv) {
    if (argc != 2) {
        fprintf(stderr, "Usgae : %s port!\n", argv[0]);
        exit(1);
    }
    int server_listen, sockfd, port;
    port = atoi(argv[1]);
    
    if ((server_listen = socket_create(port)) < 0) {
        perror("server_listen()");
        exit(1);
    }

    pid_t pid;

    while (1) {
        struct sockaddr_in client;
        struct User user;

        bzero(&user, sizeof(user));
        
        //sizeof是操作符，类似于+-×/
        socklen_t len = sizeof(client);
        if ((sockfd = accept(server_listen, (struct sockaddr *)&client, &len)) < 0)  {
            perror("accept()");
            exit(1);
        }
        get_info("../../common/names", &user, inet_ntoa(client.sin_addr)); 
        if ((pid = fork()) < 0) {
            perror("fork()");
            exit(1);
        }

        if (pid == 0) {
            worker(sockfd, &user); //如果是多线程使用user会出错,使用同一块内存
            exit(0);
        }
        //是否wait
    }

    return 0;
}
```





## one-thread单线程

![image-20210324190158287](/Image/B10.IO模型-photo/image-20210324190158287.png)

```cpp
int main(int argc, char **argv) {
    if (argc != 2) {
        fprintf(stderr, "Usage : %s port!\n", argv[0]);
        exit(1);
    }

    int server_listen, sockfd, port;
    port = atoi(argv[1]);
    if ((server_listen = socket_create(port)) < 0) {
        perror("socket_create()");
        exit(1);
    }
    
    while (1) {
        struct sockaddr_in client;
        char buff[512] = {0};
        socklen_t len = sizeof(client);
        if ((sockfd = accept(server_listen, (struct sockaddr *)&client, &len)) < 0) {
            perror("accept()");
            exit(1);
        }
        read(sockfd, buff, sizeof(buff));
        printf(BLUE"%s"NONE" : %s\n", user.name, buff);
        close(sockfd);
    }
    return 0;
}

```

![image-20210319084750441](/Image/B10.IO模型-photo/image-20210319084750441.png)

反应堆模式：IO多路复用

单线程CPU利用率低



## thread_pool_reactor模式

单独多线程需要加锁,响应时间过长

解决阻塞的方法:IO多路复用

reactor dispatch反应堆调度

Acceptor受体

Main Accpet thread主接受线程

sub-reactor thread子反应器线程

worker threads工作线程

Thread Pool线程池

![image-20210324191003817](/Image/B10.IO模型-photo/image-20210324191003817.png)



```cpp
#include "head.h"
#include "./thread_pool/thread_pool.h"
#define MAXTHREAD 10
#define MAXQUEUE 20
#define MAXEVENTS 10
#define MAXUSER 1024

int epollfd;

int main(int argc, char **argv) {
    if (argc != 2) {
        fprintf(stderr, "Usage : %s port!\n", argv[0]);
        exit(1);
    }
    int server_listen, sockfd, port;
    int fd[MAXUSER] = {0};
    port = atoi(argv[1]);
    if ((server_listen = socket_create(port)) < 0) {
        perror("socket_create()");
        exit(1);
    }

    struct task_queue taskQueue;
    task_queue_init(&taskQueue, MAXQUEUE);

    pthread_t *tid = (pthread_t *)calloc(MAXTHREAD, sizeof(pthread_t));

    for (int i = 0; i < MAXTHREAD; i++) {
        pthread_create(&tid[i], NULL, thread_run, (void *)&taskQueue);
    }

    if ((epollfd = epoll_create(1)) < 0) {
        perror("epoll_create()");
        exit(1);
    }

    struct epoll_event ev, events[MAXEVENTS];

    ev.data.fd = server_listen;
    ev.events = EPOLLIN;

    if (epoll_ctl(epollfd, EPOLL_CTL_ADD, server_listen, &ev) < 0) {
        perror("epoll_ctl()");
        exit(1);
    }
    
    while (1) {
        int nfds = epoll_wait(epollfd, events, MAXEVENTS, -1);
        if (nfds < 0) {
            perror("epoll_wait");
            exit(1);
        }
        DBG(YELLOW"<Debg>"NONE" : After wait nfds = %d\n", nfds);
        for (int i = 0; i < nfds; ++i) {
            if (events[i].data.fd == server_listen  && (events[i].events & EPOLLIN)) {
                if ((sockfd = accept(server_listen, NULL, NULL)) < 0) {
                    perror("accept()");
                    exit(1);
                }
                fd[sockfd] = sockfd;
                ev.events = EPOLLIN;
                ev.data.fd = sockfd;
                if (epoll_ctl(epollfd, EPOLL_CTL_ADD, sockfd, &ev) < 0) {
                    perror("epoll_ctl()");
                    exit(1);
                }
            } else {
                if (events[i].events & EPOLLIN) {
                    task_queue_push(&taskQueue, events[i].data.fd);
                }
                /*sleep(3);
                if (events[i].events & EPOLLHUP) {
                    epoll_ctl(epollfd, EPOLL_CTL_DEL, events[i].data.fd, NULL);
                    DBG("events[i].data.fd = %d\n", events[i].data.fd);
                    close(events[i].data.fd);
                    printf("Logout!\n");
                }
                */
            }
        }
    }
    return 0;
}

```



## 主从反应堆模式

为了解耦

根据CPU的核心数，设置反应堆的数量，最大利用CPU

![image-20210324191241195](/Image/B10.IO模型-photo/image-20210324191241195.png)

主反应堆：负责接收IO请求

从反应堆：负责处理数据

![image-20210324191529514](/Image/B10.IO模型-photo/image-20210324191529514.png)



线程池：解耦

反应堆：CPU核心数，IO多路复用

