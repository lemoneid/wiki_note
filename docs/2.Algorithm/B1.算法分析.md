---
id : B1.算法分析
title : B1.算法分析
typora-root-url : ../
---

# 算法

**算法**:对于一定规范的输入,具有明确定义的规则，能在有限时间内获得要求的输出

算法设计(策略)要不懈努力,反复修正



#### 算法的分类

1. 按求解问题的类型分组
2. 按算法内在的设计技术分组

精确算法:

近似算法:平方根,解非线性方程,求定积分



#### 算法的表示

1. 流程图
2. 伪代码
   - for while if
   - <- 赋值
   - // 注释

3. code



# 算法效率分析

### 算法效率

1. 时间复杂度:用输入规模的函数来度量
2. 空间复杂度:额外空间
3. 同等规模的输入,运行时间差距很大
   - 最差效率
   - 平均效率
   - 最有效率
4. 增长次数:当$输入规模 \to \infin$,算法运行时间表现出固定的增长次数

| 类型      | 名称     | 注释                                 | 1s内可以处理的数据规模 |
| --------- | -------- | ------------------------------------ | ---------------------- |
| $1$       | 常量     |                                      |                        |
| $log(n)$  | 对数     | 每一次循环消去问题规模的一个常熟因子 |                        |
| $n$       | 线性     | 单次循环扫描:顺序查找                | $5 * 10^8$             |
| $nlog(n)$ | 线性对数 | 分支:合并排序,快速排序               | $2*10^7$               |
| $n^2$     | 平方     |                                      |                        |
| $2^n$     | 指数     |                                      |                        |
| $n!$      | 阶乘     | 完全排列                             |                        |

#### 大O

**大O用来表示上界的**，当用它作为算法的最坏情况运行时间的上界，就是对任意数据输入的运行时间的上界。

在决定使用哪些算法的时候，不是时间复杂越低的越好（因为简化后的时间复杂度忽略了常数项等等），要**考虑数据规模**

$O(\log(n))$ 忽略底数：`以2为底n的对数 = 以2为底10的对数 * 以10为底n的对数`。

![image-20210913213338694](/Image/B1.算法分析-photo/image-20210913213338694.png)

### 时间复杂度分析：

1. 确定基本操作

2. 构造基于基本操作的函数解析式

3. 求解函数解析式

4. 如果构建的是递推关系式，那么常用的求解方法有：

   - 前向替换法：找到闭合公式，通过带入初始方程或数学归纳证明

   - 反向替换法：$x(n)=x(n-1)+n = x(n - 2) + n-1 + n$, 重复这一过程

   - 变量替换：对于$f(n) = f(n/k) + b$ 
     $$
     \begin{aligned} 
      &f(n)= f(km-1)+b\\
      = & f(km-2)+2b = \cdots \\
     = & f(k0)+mb = f(1) + blog(n)\\
     \end{aligned}
     $$



递归算法的时间复杂度怎么计算？⼦问题总数 x 每个⼦问题的时间。

递归算法的时间复杂度本质上是要看: **递归的次数 * 每次递归中的操作次数**。

#### 从硬件配置看计算机的性能

计算机的运算速度主要看CPU的配置。CPU配置：2.7 GHz Dual-Core Intel Core i5 。

1Hz = 1/s，1Hz 是CPU的一次脉冲（可以理解为一次改变状态，也叫时钟周期），称之为为赫兹。

* 1GHz（兆赫）= 1000MHz（兆赫）= 10亿Hz
* 1MHz（兆赫）= 1百万赫兹

不要简单理解一个时钟周期就是一次CPU运算。例如1 + 2 = 3，cpu要执行四次才能完整这个操作，步骤一：把1放入寄存机，步骤二：把2放入寄存器，步骤三：做加法，步骤四：保存3。

* CPU执行每条指令所需的时间实际上并不相同，例如CPU执行加法和乘法操作的耗时实际上都是不一样的。
* 内存管理都有缓存技术，所以频繁访问相同地址的数据和访问不相邻元素所需的时间也是不同的。
* 计算机同时运行多个程序，每个程序里还有不同的进程线程在抢占资源。

### 空间复杂度分析

关注空间复杂度有两个常见的相关问题

1. 空间复杂度是考虑程序（可执行文件）的大小么？
   混淆程序运行时内存大小和程序本身的大小。强调**空间复杂度是考虑程序运行时占用内存的大小，而不是可执行文件的大小。**

2. 空间复杂度是准确算出程序运行时所占用的内存么？
   不要以为空间复杂度就已经精准的掌握了程序的内存使用大小，很有多因素会影响程序真正内存使用大小，例如编译器的内存对齐，编程语言容器的底层实现等等这些都会影响到程序内存的开销。所以空间复杂度是预先大体评估程序内存使用的大小。



| Data Structure                                               | Time Complexity |             |             |             |             |             |             |             | Space Complexity |
| ------------------------------------------------------------ | --------------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ---------------- |
|                                                              | Average         |             |             |             | Worst       |             |             |             | Worst            |
|                                                              | Access          | Search      | Insertion   | Deletion    | Access      | Search      | Insertion   | Deletion    |                  |
| [Array](http://en.wikipedia.org/wiki/Array_data_structure)   | `Θ(1)`          | `Θ(n)`      | `Θ(n)`      | `Θ(n)`      | `O(1)`      | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`           |
| [Stack](http://en.wikipedia.org/wiki/Stack_(abstract_data_type)) | `Θ(n)`          | `Θ(n)`      | `Θ(1)`      | `Θ(1)`      | `O(n)`      | `O(n)`      | `O(1)`      | `O(1)`      | `O(n)`           |
| [Queue](http://en.wikipedia.org/wiki/Queue_(abstract_data_type)) | `Θ(n)`          | `Θ(n)`      | `Θ(1)`      | `Θ(1)`      | `O(n)`      | `O(n)`      | `O(1)`      | `O(1)`      | `O(n)`           |
| [Singly-Linked List](http://en.wikipedia.org/wiki/Singly_linked_list#Singly_linked_lists) | `Θ(n)`          | `Θ(n)`      | `Θ(1)`      | `Θ(1)`      | `O(n)`      | `O(n)`      | `O(1)`      | `O(1)`      | `O(n)`           |
| [Doubly-Linked List](http://en.wikipedia.org/wiki/Doubly_linked_list) | `Θ(n)`          | `Θ(n)`      | `Θ(1)`      | `Θ(1)`      | `O(n)`      | `O(n)`      | `O(1)`      | `O(1)`      | `O(n)`           |
| [Skip List](http://en.wikipedia.org/wiki/Skip_list)          | `Θ(log(n))`     | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`      | `O(n log(n))`    |
| [Hash Table](http://en.wikipedia.org/wiki/Hash_table)        | `N/A`           | `Θ(1)`      | `Θ(1)`      | `Θ(1)`      | `N/A`       | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`           |
| [Binary Search Tree](http://en.wikipedia.org/wiki/Binary_search_tree) | `Θ(log(n))`     | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`           |
| [Cartesian Tree](https://en.wikipedia.org/wiki/Cartesian_tree) | `N/A`           | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `N/A`       | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`           |
| [B-Tree](http://en.wikipedia.org/wiki/B_tree)                | `Θ(log(n))`     | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(n)`           |
| [Red-Black Tree](http://en.wikipedia.org/wiki/Red-black_tree) | `Θ(log(n))`     | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(n)`           |
| [Splay Tree](https://en.wikipedia.org/wiki/Splay_tree)       | `N/A`           | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `N/A`       | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(n)`           |
| [AVL Tree](http://en.wikipedia.org/wiki/AVL_tree)            | `Θ(log(n))`     | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(log(n))` | `O(n)`           |
| [KD Tree](http://en.wikipedia.org/wiki/K-d_tree)             | `Θ(log(n))`     | `Θ(log(n))` | `Θ(log(n))` | `Θ(log(n))` | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`      | `O(n)`           |

#### Array Sorting Algorithms

| Algorithm                                                    | Time Complexity |                  |                  | Space Complexity |
| ------------------------------------------------------------ | --------------- | ---------------- | ---------------- | ---------------- |
|                                                              | Best            | Average          | Worst            | Worst            |
| [Quicksort](http://en.wikipedia.org/wiki/Quicksort)          | `Ω(n log(n))`   | `Θ(n log(n))`    | `O(n^2)`         | `O(log(n))`      |
| [Mergesort](http://en.wikipedia.org/wiki/Merge_sort)         | `Ω(n log(n))`   | `Θ(n log(n))`    | `O(n log(n))`    | `O(n)`           |
| [Timsort](http://en.wikipedia.org/wiki/Timsort)              | `Ω(n)`          | `Θ(n log(n))`    | `O(n log(n))`    | `O(n)`           |
| [Heapsort](http://en.wikipedia.org/wiki/Heapsort)            | `Ω(n log(n))`   | `Θ(n log(n))`    | `O(n log(n))`    | `O(1)`           |
| [Bubble Sort](http://en.wikipedia.org/wiki/Bubble_sort)      | `Ω(n)`          | `Θ(n^2)`         | `O(n^2)`         | `O(1)`           |
| [Insertion Sort](http://en.wikipedia.org/wiki/Insertion_sort) | `Ω(n)`          | `Θ(n^2)`         | `O(n^2)`         | `O(1)`           |
| [Selection Sort](http://en.wikipedia.org/wiki/Selection_sort) | `Ω(n^2)`        | `Θ(n^2)`         | `O(n^2)`         | `O(1)`           |
| [Tree Sort](https://en.wikipedia.org/wiki/Tree_sort)         | `Ω(n log(n))`   | `Θ(n log(n))`    | `O(n^2)`         | `O(n)`           |
| [Shell Sort](http://en.wikipedia.org/wiki/Shellsort)         | `Ω(n log(n))`   | `Θ(n(log(n))^2)` | `O(n(log(n))^2)` | `O(1)`           |
| [Bucket Sort](http://en.wikipedia.org/wiki/Bucket_sort)      | `Ω(n+k)`        | `Θ(n+k)`         | `O(n^2)`         | `O(n)`           |
| [Radix Sort](http://en.wikipedia.org/wiki/Radix_sort)        | `Ω(nk)`         | `Θ(nk)`          | `O(nk)`          | `O(n+k)`         |
| [Counting Sort](https://en.wikipedia.org/wiki/Counting_sort) | `Ω(n+k)`        | `Θ(n+k)`         | `O(n+k)`         | `O(k)`           |
| [Cubesort](https://en.wikipedia.org/wiki/Cubesort)           | `Ω(n)`          | `Θ(n log(n))`    | `O(n log(n))`    | `O(n)`           |



#### 伪随机数

1. 线性同余法

## 基本数据结构和算法

### 链表

* 链表
* 双向链表

### 哈希表/散列表 (Hash Table)

* 散列函数
* 碰撞解决

### 字符串算法  

* 排序
* 查找
  * BF算法  
  * KMP算法  
  * BM算法  
* 正则表达式
* 数据压缩


### 树

* 二叉树    
* 二叉查找树   
* 伸展树(splay tree 分裂树)   
* 平衡二叉树AVL    
* 红黑树  
* B树,B+,B*  
* R树  
* Trie树(前缀树)  
* 后缀树  
* 最优二叉树(赫夫曼树) 
* 二叉堆 （大根堆，小根堆）   
* 二项树    
* 二项堆  
* 斐波那契堆(Fibonacci Heap)   


### 图的算法

* 图的存储结构和基本操作（建立，遍历，删除节点，添加节点）   

* 最小生成树  

* 拓扑排序  

* 关键路径  

* 最短路径: Floyd,Dijkstra,bellman-ford,spfa  

  

### 排序算法

**交换排序算法**

* 冒泡排序
* 插入排序    
* 选择排序    
* 希尔排序
* 快排   
* 归并排序  
* 堆排序

**线性排序算法**
    

* 桶排序 

  

### 查找算法  

* 顺序表查找：顺序查找  
* 有序表查找：二分查找  
* 分块查找： 块内无序，块之间有序；可以先二分查找定位到块，然后再到`块`中顺序查找  
* 动态查找:  二叉排序树，AVL树，B- ，B+    （这里之所以叫 `动态查找表`，是因为表结构是查找的过程中动态生成的）
* 哈希表：  O(1)     


### 15个经典基础算法

* Hash  
* 快速排序 
* 快递选择SELECT 
* BFS/DFS （广度/深度优先遍历）    
* 红黑树 （一种自平衡的`二叉查找树`）  
* KMP    字符串匹配算法
* DP (动态规划 dynamic programming)   
* A*寻路算法： 求解最短路径 
* Dijkstra：最短路径算法 （八卦下：Dijkstra是荷兰的计算机科学家,提出”信号量和PV原语“,"解决哲学家就餐问题",”死锁“也是它提出来的） 
* 遗传算法  
* 启发式搜索   
* 图像特征提取之SIFT算法  
* 傅立叶变换  
* SPFA(shortest path faster algorithm)  单元最短路径算法  


## 海量数据处理

* Hash映射/分而治之
* Bitmap
* Bloom filter(布隆过滤器)
* Trie树
* 数据库索引
* 倒排索引(Inverted Index)
* 双层桶划分
* 外排序
* simhash算法
* 分布处理之Mapreduce


## 算法设计思想

* 迭代法  
* 穷举搜索法  
* 递推法  
* 动态规划  
* 贪心算法  
* 回溯  
* 分治算法  

